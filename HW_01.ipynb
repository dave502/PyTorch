{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\programdata\\anaconda3\\lib\\site-packages (1.11.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.10.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch import nn \n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создайте случайный FloatTensor размера 3x4x5\n",
    "tensor = torch.FloatTensor(torch.rand(3,4,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4, 5])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Выведите его форму (shape)\n",
    "tensor.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3814, 0.5191, 0.9181, 0.3312, 0.5767, 0.4381, 0.8117, 0.9652, 0.9546,\n",
       "         0.6559],\n",
       "        [0.3887, 0.6700, 0.1186, 0.7295, 0.9962, 0.7074, 0.6006, 0.4262, 0.0868,\n",
       "         0.8623],\n",
       "        [0.0958, 0.0831, 0.2047, 0.8418, 0.3804, 0.4521, 0.5120, 0.8677, 0.3540,\n",
       "         0.9467],\n",
       "        [0.0221, 0.1906, 0.5844, 0.0217, 0.6655, 0.0836, 0.8308, 0.7053, 0.8447,\n",
       "         0.9559],\n",
       "        [0.1824, 0.9476, 0.9918, 0.3257, 0.5760, 0.9610, 0.7269, 0.1508, 0.0216,\n",
       "         0.6894],\n",
       "        [0.1121, 0.7228, 0.5285, 0.1532, 0.2445, 0.8619, 0.4431, 0.3858, 0.5500,\n",
       "         0.0882]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Приведите его к форме 6 X 10\n",
    "tensor = tensor.view(6,10)\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3814, 2.0763, 1.8361, 0.6625, 0.5767, 0.4381, 3.2468, 1.9305, 1.9091,\n",
       "         0.6559],\n",
       "        [0.3887, 2.6798, 0.2372, 1.4591, 0.9962, 0.7074, 2.4025, 0.8524, 0.1736,\n",
       "         0.8623],\n",
       "        [0.0958, 0.3325, 0.4093, 1.6835, 0.3804, 0.4521, 2.0479, 1.7354, 0.7080,\n",
       "         0.9467],\n",
       "        [0.0221, 0.7625, 1.1687, 0.0435, 0.6655, 0.0836, 3.3233, 1.4105, 1.6894,\n",
       "         0.9559],\n",
       "        [0.1824, 3.7904, 1.9835, 0.6513, 0.5760, 0.9610, 2.9077, 0.3016, 0.0431,\n",
       "         0.6894],\n",
       "        [0.1121, 2.8911, 1.0570, 0.3063, 0.2445, 0.8619, 1.7723, 0.7716, 1.1000,\n",
       "         0.0882]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Умножьте его на вектор [1, 4, 2, 2, 1] поэлементно\n",
    "# тут возможно два варианта поэлементного умножения: перебирать элементы множимого и умножать их\n",
    "# на вектор или такой (результат будет, конечно, разным)\n",
    "tensor = (tensor.view(-1, 5) * torch.FloatTensor([1, 4, 2, 2, 1])).view(6,10)\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[27.1345, 18.3414, 14.9829, 21.5519, 23.3236, 18.1096],\n",
       "        [18.3414, 18.2823, 11.5188, 13.4184, 20.7475, 14.5247],\n",
       "        [14.9829, 11.5188, 12.0736, 12.4527, 11.0009,  8.2339],\n",
       "        [21.5519, 13.4184, 12.4527, 19.2010, 16.5246, 12.6111],\n",
       "        [23.3236, 20.7475, 11.0009, 16.5246, 29.0372, 19.7385],\n",
       "        [18.1096, 14.5247,  8.2339, 12.6111, 19.7385, 15.3390]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Умножьте тензор матрично на себя, чтобы результат был размерности 6x6\n",
    "torch.mm(tensor, tensor.transpose(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.grad.item()=3.0\n",
      "z.grad.item()=1.0\n",
      "t.grad.item()=-75.0\n"
     ]
    }
   ],
   "source": [
    "# Посчитайте производную функции y = x**3 + z - 75t в точке (1, 0.5, 2)\n",
    "x = torch.tensor(1.0, requires_grad = True)\n",
    "z = torch.tensor(0.5, requires_grad = True)\n",
    "t = torch.tensor(2.0, requires_grad = True)\n",
    "y = x**3 + z - 75*t\n",
    "y.backward()\n",
    "print(f'{x.grad.item()=}\\n{z.grad.item()=}\\n{t.grad.item()=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Создайте единичный тензор размера 5x6\n",
    "ones = torch.ones(5,6)\n",
    "ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Переведите его в формат numpy\n",
    "type(ones.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Давайте теперь пооптимизируем: возьмите функцию y = x**w1 - 2 * x**2 + 5\n",
    "# Посчитайте "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(w, x):\n",
    "    return x**w1 - 2*x**2 + 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Variable(torch.randn(5), requires_grad=True)\n",
    "w1 = 2 #2.345667"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4.7424, -0.1352,  4.1997,  3.8636,  4.5522])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = Variable(torch.tensor([func(w1, xi) for xi in x]), requires_grad=False) \n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.7165], requires_grad=True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1 = torch.randn(1)\n",
    "w1.requires_grad_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD([w1], lr=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:529: UserWarning: Using a target size (torch.Size([])) that is different to the input size (torch.Size([1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  1.871405839920044\n",
      "iteration 0, w1=tensor([2.2996], requires_grad=True), y=tensor([4.6950], grad_fn=<AddBackward0>)\n",
      "loss:  0.002243777271360159\n",
      "iteration 1, w1=tensor([2.2861], requires_grad=True), y=tensor([4.6969], grad_fn=<AddBackward0>)\n",
      "loss:  0.0020642480812966824\n",
      "iteration 2, w1=tensor([2.2730], requires_grad=True), y=tensor([4.6988], grad_fn=<AddBackward0>)\n",
      "loss:  0.0018961053574457765\n",
      "iteration 3, w1=tensor([2.2604], requires_grad=True), y=tensor([4.7007], grad_fn=<AddBackward0>)\n",
      "loss:  0.0017389600398018956\n",
      "iteration 4, w1=tensor([2.2482], requires_grad=True), y=tensor([4.7025], grad_fn=<AddBackward0>)\n",
      "loss:  0.00159245275426656\n",
      "iteration 5, w1=tensor([2.2364], requires_grad=True), y=tensor([4.7042], grad_fn=<AddBackward0>)\n",
      "loss:  0.0014561011921614408\n",
      "iteration 6, w1=tensor([2.2250], requires_grad=True), y=tensor([4.7059], grad_fn=<AddBackward0>)\n",
      "loss:  0.0013294650707393885\n",
      "iteration 7, w1=tensor([2.2141], requires_grad=True), y=tensor([4.7076], grad_fn=<AddBackward0>)\n",
      "loss:  0.0012121058534830809\n",
      "iteration 8, w1=tensor([2.2036], requires_grad=True), y=tensor([4.7092], grad_fn=<AddBackward0>)\n",
      "loss:  0.0011035877978429198\n",
      "iteration 9, w1=tensor([2.1935], requires_grad=True), y=tensor([4.7107], grad_fn=<AddBackward0>)\n",
      "loss:  0.0010033875005319715\n",
      "iteration 10, w1=tensor([2.1838], requires_grad=True), y=tensor([4.7122], grad_fn=<AddBackward0>)\n",
      "loss:  0.0009110901155509055\n",
      "iteration 11, w1=tensor([2.1745], requires_grad=True), y=tensor([4.7136], grad_fn=<AddBackward0>)\n",
      "loss:  0.0008262027986347675\n",
      "iteration 12, w1=tensor([2.1655], requires_grad=True), y=tensor([4.7150], grad_fn=<AddBackward0>)\n",
      "loss:  0.0007482805522158742\n",
      "iteration 13, w1=tensor([2.1570], requires_grad=True), y=tensor([4.7164], grad_fn=<AddBackward0>)\n",
      "loss:  0.0006768692983314395\n",
      "iteration 14, w1=tensor([2.1488], requires_grad=True), y=tensor([4.7177], grad_fn=<AddBackward0>)\n",
      "loss:  0.0006115594878792763\n",
      "iteration 15, w1=tensor([2.1410], requires_grad=True), y=tensor([4.7189], grad_fn=<AddBackward0>)\n",
      "loss:  0.0005519122933037579\n",
      "iteration 16, w1=tensor([2.1335], requires_grad=True), y=tensor([4.7201], grad_fn=<AddBackward0>)\n",
      "loss:  0.0004975348128937185\n",
      "iteration 17, w1=tensor([2.1264], requires_grad=True), y=tensor([4.7212], grad_fn=<AddBackward0>)\n",
      "loss:  0.0004480334755498916\n",
      "iteration 18, w1=tensor([2.1196], requires_grad=True), y=tensor([4.7223], grad_fn=<AddBackward0>)\n",
      "loss:  0.0004030376730952412\n",
      "iteration 19, w1=tensor([2.1132], requires_grad=True), y=tensor([4.7233], grad_fn=<AddBackward0>)\n",
      "loss:  0.00036219891626387835\n",
      "iteration 20, w1=tensor([2.1070], requires_grad=True), y=tensor([4.7243], grad_fn=<AddBackward0>)\n",
      "loss:  0.0003251900780014694\n",
      "iteration 21, w1=tensor([2.1012], requires_grad=True), y=tensor([4.7253], grad_fn=<AddBackward0>)\n",
      "loss:  0.00029168801847845316\n",
      "iteration 22, w1=tensor([2.0956], requires_grad=True), y=tensor([4.7262], grad_fn=<AddBackward0>)\n",
      "loss:  0.00026142343995161355\n",
      "iteration 23, w1=tensor([2.0903], requires_grad=True), y=tensor([4.7271], grad_fn=<AddBackward0>)\n",
      "loss:  0.00023408379638567567\n",
      "iteration 24, w1=tensor([2.0853], requires_grad=True), y=tensor([4.7279], grad_fn=<AddBackward0>)\n",
      "loss:  0.00020945281721651554\n",
      "iteration 25, w1=tensor([2.0805], requires_grad=True), y=tensor([4.7287], grad_fn=<AddBackward0>)\n",
      "loss:  0.00018725932750385255\n",
      "iteration 26, w1=tensor([2.0760], requires_grad=True), y=tensor([4.7294], grad_fn=<AddBackward0>)\n",
      "loss:  0.00016728139598853886\n",
      "iteration 27, w1=tensor([2.0717], requires_grad=True), y=tensor([4.7302], grad_fn=<AddBackward0>)\n",
      "loss:  0.00014934940554667264\n",
      "iteration 28, w1=tensor([2.0676], requires_grad=True), y=tensor([4.7308], grad_fn=<AddBackward0>)\n",
      "loss:  0.00013323617167770863\n",
      "iteration 29, w1=tensor([2.0637], requires_grad=True), y=tensor([4.7315], grad_fn=<AddBackward0>)\n",
      "loss:  0.00011878966324729845\n",
      "iteration 30, w1=tensor([2.0601], requires_grad=True), y=tensor([4.7321], grad_fn=<AddBackward0>)\n",
      "loss:  0.00010584785195533186\n",
      "iteration 31, w1=tensor([2.0566], requires_grad=True), y=tensor([4.7327], grad_fn=<AddBackward0>)\n",
      "loss:  9.426238102605566e-05\n",
      "iteration 32, w1=tensor([2.0534], requires_grad=True), y=tensor([4.7332], grad_fn=<AddBackward0>)\n",
      "loss:  8.388889546040446e-05\n",
      "iteration 33, w1=tensor([2.0503], requires_grad=True), y=tensor([4.7337], grad_fn=<AddBackward0>)\n",
      "loss:  7.462988287443295e-05\n",
      "iteration 34, w1=tensor([2.0474], requires_grad=True), y=tensor([4.7342], grad_fn=<AddBackward0>)\n",
      "loss:  6.635420868406072e-05\n",
      "iteration 35, w1=tensor([2.0446], requires_grad=True), y=tensor([4.7347], grad_fn=<AddBackward0>)\n",
      "loss:  5.896681977901608e-05\n",
      "iteration 36, w1=tensor([2.0420], requires_grad=True), y=tensor([4.7351], grad_fn=<AddBackward0>)\n",
      "loss:  5.2380455599632114e-05\n",
      "iteration 37, w1=tensor([2.0396], requires_grad=True), y=tensor([4.7356], grad_fn=<AddBackward0>)\n",
      "loss:  4.650864866562188e-05\n",
      "iteration 38, w1=tensor([2.0372], requires_grad=True), y=tensor([4.7360], grad_fn=<AddBackward0>)\n",
      "loss:  4.127938882447779e-05\n",
      "iteration 39, w1=tensor([2.0351], requires_grad=True), y=tensor([4.7363], grad_fn=<AddBackward0>)\n",
      "loss:  3.662691233330406e-05\n",
      "iteration 40, w1=tensor([2.0330], requires_grad=True), y=tensor([4.7367], grad_fn=<AddBackward0>)\n",
      "loss:  3.2480398658663034e-05\n",
      "iteration 41, w1=tensor([2.0310], requires_grad=True), y=tensor([4.7370], grad_fn=<AddBackward0>)\n",
      "loss:  2.8797447157558054e-05\n",
      "iteration 42, w1=tensor([2.0292], requires_grad=True), y=tensor([4.7373], grad_fn=<AddBackward0>)\n",
      "loss:  2.5523610020172782e-05\n",
      "iteration 43, w1=tensor([2.0275], requires_grad=True), y=tensor([4.7376], grad_fn=<AddBackward0>)\n",
      "loss:  2.2614751287619583e-05\n",
      "iteration 44, w1=tensor([2.0258], requires_grad=True), y=tensor([4.7379], grad_fn=<AddBackward0>)\n",
      "loss:  2.003093686653301e-05\n",
      "iteration 45, w1=tensor([2.0243], requires_grad=True), y=tensor([4.7382], grad_fn=<AddBackward0>)\n",
      "loss:  1.7740123439580202e-05\n",
      "iteration 46, w1=tensor([2.0229], requires_grad=True), y=tensor([4.7384], grad_fn=<AddBackward0>)\n",
      "loss:  1.5705318219261244e-05\n",
      "iteration 47, w1=tensor([2.0215], requires_grad=True), y=tensor([4.7387], grad_fn=<AddBackward0>)\n",
      "loss:  1.3900889825890772e-05\n",
      "iteration 48, w1=tensor([2.0202], requires_grad=True), y=tensor([4.7389], grad_fn=<AddBackward0>)\n",
      "loss:  1.2300011803745292e-05\n",
      "iteration 49, w1=tensor([2.0190], requires_grad=True), y=tensor([4.7391], grad_fn=<AddBackward0>)\n",
      "loss:  1.088181397790322e-05\n",
      "iteration 50, w1=tensor([2.0179], requires_grad=True), y=tensor([4.7393], grad_fn=<AddBackward0>)\n",
      "loss:  9.624281119613443e-06\n",
      "iteration 51, w1=tensor([2.0168], requires_grad=True), y=tensor([4.7395], grad_fn=<AddBackward0>)\n",
      "loss:  8.510579391440842e-06\n",
      "iteration 52, w1=tensor([2.0158], requires_grad=True), y=tensor([4.7396], grad_fn=<AddBackward0>)\n",
      "loss:  7.525388355134055e-06\n",
      "iteration 53, w1=tensor([2.0148], requires_grad=True), y=tensor([4.7398], grad_fn=<AddBackward0>)\n",
      "loss:  6.65479547024006e-06\n",
      "iteration 54, w1=tensor([2.0139], requires_grad=True), y=tensor([4.7400], grad_fn=<AddBackward0>)\n",
      "loss:  5.881564902665559e-06\n",
      "iteration 55, w1=tensor([2.0131], requires_grad=True), y=tensor([4.7401], grad_fn=<AddBackward0>)\n",
      "loss:  5.197298378334381e-06\n",
      "iteration 56, w1=tensor([2.0123], requires_grad=True), y=tensor([4.7402], grad_fn=<AddBackward0>)\n",
      "loss:  4.592046934703831e-06\n",
      "iteration 57, w1=tensor([2.0116], requires_grad=True), y=tensor([4.7404], grad_fn=<AddBackward0>)\n",
      "loss:  4.056841135025024e-06\n",
      "iteration 58, w1=tensor([2.0109], requires_grad=True), y=tensor([4.7405], grad_fn=<AddBackward0>)\n",
      "loss:  3.585419335649931e-06\n",
      "iteration 59, w1=tensor([2.0102], requires_grad=True), y=tensor([4.7406], grad_fn=<AddBackward0>)\n",
      "loss:  3.166820533806458e-06\n",
      "iteration 60, w1=tensor([2.0096], requires_grad=True), y=tensor([4.7407], grad_fn=<AddBackward0>)\n",
      "loss:  2.796479975586408e-06\n",
      "iteration 61, w1=tensor([2.0090], requires_grad=True), y=tensor([4.7408], grad_fn=<AddBackward0>)\n",
      "loss:  2.470100298523903e-06\n",
      "iteration 62, w1=tensor([2.0085], requires_grad=True), y=tensor([4.7409], grad_fn=<AddBackward0>)\n",
      "loss:  2.180833917009295e-06\n",
      "iteration 63, w1=tensor([2.0080], requires_grad=True), y=tensor([4.7410], grad_fn=<AddBackward0>)\n",
      "loss:  1.9254230210208334e-06\n",
      "iteration 64, w1=tensor([2.0075], requires_grad=True), y=tensor([4.7411], grad_fn=<AddBackward0>)\n",
      "loss:  1.6995627447613515e-06\n",
      "iteration 65, w1=tensor([2.0070], requires_grad=True), y=tensor([4.7412], grad_fn=<AddBackward0>)\n",
      "loss:  1.5006119156169007e-06\n",
      "iteration 66, w1=tensor([2.0066], requires_grad=True), y=tensor([4.7412], grad_fn=<AddBackward0>)\n",
      "loss:  1.3238989140518242e-06\n",
      "iteration 67, w1=tensor([2.0062], requires_grad=True), y=tensor([4.7413], grad_fn=<AddBackward0>)\n",
      "loss:  1.1685390290949726e-06\n",
      "iteration 68, w1=tensor([2.0058], requires_grad=True), y=tensor([4.7414], grad_fn=<AddBackward0>)\n",
      "loss:  1.0315716281183995e-06\n",
      "iteration 69, w1=tensor([2.0055], requires_grad=True), y=tensor([4.7414], grad_fn=<AddBackward0>)\n",
      "loss:  9.104044238483766e-07\n",
      "iteration 70, w1=tensor([2.0051], requires_grad=True), y=tensor([4.7415], grad_fn=<AddBackward0>)\n",
      "loss:  8.036295184865594e-07\n",
      "iteration 71, w1=tensor([2.0048], requires_grad=True), y=tensor([4.7415], grad_fn=<AddBackward0>)\n",
      "loss:  7.091230145306326e-07\n",
      "iteration 72, w1=tensor([2.0045], requires_grad=True), y=tensor([4.7416], grad_fn=<AddBackward0>)\n",
      "loss:  6.257962468225742e-07\n",
      "iteration 73, w1=tensor([2.0043], requires_grad=True), y=tensor([4.7416], grad_fn=<AddBackward0>)\n",
      "loss:  5.519186743185855e-07\n",
      "iteration 74, w1=tensor([2.0040], requires_grad=True), y=tensor([4.7417], grad_fn=<AddBackward0>)\n",
      "loss:  4.873290890827775e-07\n",
      "iteration 75, w1=tensor([2.0038], requires_grad=True), y=tensor([4.7417], grad_fn=<AddBackward0>)\n",
      "loss:  4.298783551348606e-07\n",
      "iteration 76, w1=tensor([2.0035], requires_grad=True), y=tensor([4.7418], grad_fn=<AddBackward0>)\n",
      "loss:  3.7895938476140145e-07\n",
      "iteration 77, w1=tensor([2.0033], requires_grad=True), y=tensor([4.7418], grad_fn=<AddBackward0>)\n",
      "loss:  3.345505774632329e-07\n",
      "iteration 78, w1=tensor([2.0031], requires_grad=True), y=tensor([4.7418], grad_fn=<AddBackward0>)\n",
      "loss:  2.9497664399968926e-07\n",
      "iteration 79, w1=tensor([2.0029], requires_grad=True), y=tensor([4.7419], grad_fn=<AddBackward0>)\n",
      "loss:  2.603201210149564e-07\n",
      "iteration 80, w1=tensor([2.0027], requires_grad=True), y=tensor([4.7419], grad_fn=<AddBackward0>)\n",
      "loss:  2.2965309653955046e-07\n",
      "iteration 81, w1=tensor([2.0026], requires_grad=True), y=tensor([4.7419], grad_fn=<AddBackward0>)\n",
      "loss:  2.0262086763978004e-07\n",
      "iteration 82, w1=tensor([2.0024], requires_grad=True), y=tensor([4.7420], grad_fn=<AddBackward0>)\n",
      "loss:  1.784874257282354e-07\n",
      "iteration 83, w1=tensor([2.0023], requires_grad=True), y=tensor([4.7420], grad_fn=<AddBackward0>)\n",
      "loss:  1.5739351511001587e-07\n",
      "iteration 84, w1=tensor([2.0021], requires_grad=True), y=tensor([4.7420], grad_fn=<AddBackward0>)\n",
      "loss:  1.3904445950174704e-07\n",
      "iteration 85, w1=tensor([2.0020], requires_grad=True), y=tensor([4.7420], grad_fn=<AddBackward0>)\n",
      "loss:  1.2249893188709393e-07\n",
      "iteration 86, w1=tensor([2.0019], requires_grad=True), y=tensor([4.7421], grad_fn=<AddBackward0>)\n",
      "loss:  1.0793905858008657e-07\n",
      "iteration 87, w1=tensor([2.0018], requires_grad=True), y=tensor([4.7421], grad_fn=<AddBackward0>)\n",
      "loss:  9.518066690361593e-08\n",
      "iteration 88, w1=tensor([2.0017], requires_grad=True), y=tensor([4.7421], grad_fn=<AddBackward0>)\n",
      "loss:  8.405186235904694e-08\n",
      "iteration 89, w1=tensor([2.0016], requires_grad=True), y=tensor([4.7421], grad_fn=<AddBackward0>)\n",
      "loss:  7.413314051518682e-08\n",
      "iteration 90, w1=tensor([2.0015], requires_grad=True), y=tensor([4.7421], grad_fn=<AddBackward0>)\n",
      "loss:  6.53235474601388e-08\n",
      "iteration 91, w1=tensor([2.0014], requires_grad=True), y=tensor([4.7421], grad_fn=<AddBackward0>)\n",
      "loss:  5.75275862502167e-08\n",
      "iteration 92, w1=tensor([2.0013], requires_grad=True), y=tensor([4.7422], grad_fn=<AddBackward0>)\n",
      "loss:  5.0870085033238865e-08\n",
      "iteration 93, w1=tensor([2.0012], requires_grad=True), y=tensor([4.7422], grad_fn=<AddBackward0>)\n",
      "loss:  4.4823536882176995e-08\n",
      "iteration 94, w1=tensor([2.0011], requires_grad=True), y=tensor([4.7422], grad_fn=<AddBackward0>)\n",
      "loss:  3.953778104914818e-08\n",
      "iteration 95, w1=tensor([2.0011], requires_grad=True), y=tensor([4.7422], grad_fn=<AddBackward0>)\n",
      "loss:  3.493914846330881e-08\n",
      "iteration 96, w1=tensor([2.0010], requires_grad=True), y=tensor([4.7422], grad_fn=<AddBackward0>)\n",
      "loss:  3.079185262322426e-08\n",
      "iteration 97, w1=tensor([2.0009], requires_grad=True), y=tensor([4.7422], grad_fn=<AddBackward0>)\n",
      "loss:  2.7063151719630696e-08\n",
      "iteration 98, w1=tensor([2.0009], requires_grad=True), y=tensor([4.7422], grad_fn=<AddBackward0>)\n",
      "loss:  2.386877895332873e-08\n",
      "iteration 99, w1=tensor([2.0008], requires_grad=True), y=tensor([4.7422], grad_fn=<AddBackward0>)\n",
      "loss:  2.1012965589761734e-08\n",
      "iteration 100, w1=tensor([2.0008], requires_grad=True), y=tensor([4.7422], grad_fn=<AddBackward0>)\n",
      "loss:  1.859825715655461e-08\n",
      "iteration 101, w1=tensor([2.0007], requires_grad=True), y=tensor([4.7423], grad_fn=<AddBackward0>)\n",
      "loss:  1.63308868650347e-08\n",
      "iteration 102, w1=tensor([2.0007], requires_grad=True), y=tensor([4.7423], grad_fn=<AddBackward0>)\n",
      "loss:  1.4439137885347009e-08\n",
      "iteration 103, w1=tensor([2.0006], requires_grad=True), y=tensor([4.7423], grad_fn=<AddBackward0>)\n",
      "loss:  1.2771351975970902e-08\n",
      "iteration 104, w1=tensor([2.0006], requires_grad=True), y=tensor([4.7423], grad_fn=<AddBackward0>)\n",
      "loss:  1.1205884220544249e-08\n",
      "iteration 105, w1=tensor([2.0006], requires_grad=True), y=tensor([4.7423], grad_fn=<AddBackward0>)\n",
      "loss:  9.93190951703582e-09\n",
      "iteration 106, w1=tensor([2.0005], requires_grad=True), y=tensor([4.7423], grad_fn=<AddBackward0>)\n",
      "loss:  8.734787115827203e-09\n",
      "iteration 107, w1=tensor([2.0005], requires_grad=True), y=tensor([4.7423], grad_fn=<AddBackward0>)\n",
      "loss:  7.697963155806065e-09\n",
      "iteration 108, w1=tensor([2.0005], requires_grad=True), y=tensor([4.7423], grad_fn=<AddBackward0>)\n",
      "loss:  6.805066732340492e-09\n",
      "iteration 109, w1=tensor([2.0004], requires_grad=True), y=tensor([4.7423], grad_fn=<AddBackward0>)\n",
      "loss:  5.967194738332182e-09\n",
      "iteration 110, w1=tensor([2.0004], requires_grad=True), y=tensor([4.7423], grad_fn=<AddBackward0>)\n",
      "loss:  5.2532413974404335e-09\n",
      "iteration 111, w1=tensor([2.0004], requires_grad=True), y=tensor([4.7423], grad_fn=<AddBackward0>)\n",
      "loss:  4.649564289138652e-09\n",
      "iteration 112, w1=tensor([2.0004], requires_grad=True), y=tensor([4.7423], grad_fn=<AddBackward0>)\n",
      "loss:  4.082721716258675e-09\n",
      "iteration 113, w1=tensor([2.0003], requires_grad=True), y=tensor([4.7423], grad_fn=<AddBackward0>)\n",
      "loss:  3.609784471336752e-09\n",
      "iteration 114, w1=tensor([2.0003], requires_grad=True), y=tensor([4.7423], grad_fn=<AddBackward0>)\n",
      "loss:  3.165951056871563e-09\n",
      "iteration 115, w1=tensor([2.0003], requires_grad=True), y=tensor([4.7423], grad_fn=<AddBackward0>)\n",
      "loss:  2.801471055136062e-09\n",
      "iteration 116, w1=tensor([2.0003], requires_grad=True), y=tensor([4.7423], grad_fn=<AddBackward0>)\n",
      "loss:  2.459273673593998e-09\n",
      "iteration 117, w1=tensor([2.0003], requires_grad=True), y=tensor([4.7423], grad_fn=<AddBackward0>)\n",
      "loss:  2.1836967789568007e-09\n",
      "iteration 118, w1=tensor([2.0003], requires_grad=True), y=tensor([4.7423], grad_fn=<AddBackward0>)\n",
      "loss:  1.924490788951516e-09\n",
      "iteration 119, w1=tensor([2.0002], requires_grad=True), y=tensor([4.7423], grad_fn=<AddBackward0>)\n",
      "loss:  1.7209913494298235e-09\n",
      "iteration 120, w1=tensor([2.0002], requires_grad=True), y=tensor([4.7423], grad_fn=<AddBackward0>)\n",
      "loss:  1.4917986845830455e-09\n",
      "iteration 121, w1=tensor([2.0002], requires_grad=True), y=tensor([4.7423], grad_fn=<AddBackward0>)\n",
      "loss:  1.3133103493601084e-09\n",
      "iteration 122, w1=tensor([2.0002], requires_grad=True), y=tensor([4.7423], grad_fn=<AddBackward0>)\n",
      "loss:  1.178705133497715e-09\n",
      "iteration 123, w1=tensor([2.0002], requires_grad=True), y=tensor([4.7423], grad_fn=<AddBackward0>)\n",
      "loss:  1.0206804290646687e-09\n",
      "iteration 124, w1=tensor([2.0002], requires_grad=True), y=tensor([4.7424], grad_fn=<AddBackward0>)\n",
      "loss:  9.02446117834188e-10\n",
      "iteration 125, w1=tensor([2.0002], requires_grad=True), y=tensor([4.7424], grad_fn=<AddBackward0>)\n",
      "loss:  7.914877642178908e-10\n",
      "iteration 126, w1=tensor([2.0002], requires_grad=True), y=tensor([4.7424], grad_fn=<AddBackward0>)\n",
      "loss:  7.130438461899757e-10\n",
      "iteration 127, w1=tensor([2.0001], requires_grad=True), y=tensor([4.7424], grad_fn=<AddBackward0>)\n",
      "loss:  6.148184183984995e-10\n",
      "iteration 128, w1=tensor([2.0001], requires_grad=True), y=tensor([4.7424], grad_fn=<AddBackward0>)\n",
      "loss:  5.459241947392002e-10\n",
      "iteration 129, w1=tensor([2.0001], requires_grad=True), y=tensor([4.7424], grad_fn=<AddBackward0>)\n",
      "loss:  4.81122697237879e-10\n",
      "iteration 130, w1=tensor([2.0001], requires_grad=True), y=tensor([4.7424], grad_fn=<AddBackward0>)\n",
      "loss:  4.204139258945361e-10\n",
      "iteration 131, w1=tensor([2.0001], requires_grad=True), y=tensor([4.7424], grad_fn=<AddBackward0>)\n",
      "loss:  3.822151484200731e-10\n",
      "iteration 132, w1=tensor([2.0001], requires_grad=True), y=tensor([4.7424], grad_fn=<AddBackward0>)\n",
      "loss:  3.283275873400271e-10\n",
      "iteration 133, w1=tensor([2.0001], requires_grad=True), y=tensor([4.7424], grad_fn=<AddBackward0>)\n",
      "loss:  2.9467628337442875e-10\n",
      "iteration 134, w1=tensor([2.0001], requires_grad=True), y=tensor([4.7424], grad_fn=<AddBackward0>)\n",
      "loss:  2.6284396881237626e-10\n",
      "iteration 135, w1=tensor([2.0001], requires_grad=True), y=tensor([4.7424], grad_fn=<AddBackward0>)\n",
      "loss:  2.3283064365386963e-10\n",
      "iteration 136, w1=tensor([2.0001], requires_grad=True), y=tensor([4.7424], grad_fn=<AddBackward0>)\n",
      "loss:  2.0463630789890885e-10\n",
      "iteration 137, w1=tensor([2.0001], requires_grad=True), y=tensor([4.7424], grad_fn=<AddBackward0>)\n",
      "loss:  1.7826096154749393e-10\n",
      "iteration 138, w1=tensor([2.0001], requires_grad=True), y=tensor([4.7424], grad_fn=<AddBackward0>)\n",
      "loss:  1.5370460459962487e-10\n",
      "iteration 139, w1=tensor([2.0001], requires_grad=True), y=tensor([4.7424], grad_fn=<AddBackward0>)\n",
      "loss:  1.4210854715202004e-10\n",
      "iteration 140, w1=tensor([2.0001], requires_grad=True), y=tensor([4.7424], grad_fn=<AddBackward0>)\n",
      "loss:  1.2028067430946976e-10\n",
      "iteration 141, w1=tensor([2.0001], requires_grad=True), y=tensor([4.7424], grad_fn=<AddBackward0>)\n",
      "loss:  1.1004885891452432e-10\n",
      "iteration 142, w1=tensor([2.0001], requires_grad=True), y=tensor([4.7424], grad_fn=<AddBackward0>)\n",
      "loss:  9.094947017729282e-11\n",
      "iteration 143, w1=tensor([2.0001], requires_grad=True), y=tensor([4.7424], grad_fn=<AddBackward0>)\n",
      "loss:  8.208189683500677e-11\n",
      "iteration 144, w1=tensor([2.0000], requires_grad=True), y=tensor([4.7424], grad_fn=<AddBackward0>)\n",
      "loss:  7.366907084360719e-11\n",
      "iteration 145, w1=tensor([2.0000], requires_grad=True), y=tensor([4.7424], grad_fn=<AddBackward0>)\n",
      "loss:  6.571099220309407e-11\n",
      "iteration 146, w1=tensor([2.0000], requires_grad=True), y=tensor([4.7424], grad_fn=<AddBackward0>)\n",
      "loss:  5.820766091346741e-11\n",
      "iteration 147, w1=tensor([2.0000], requires_grad=True), y=tensor([4.7424], grad_fn=<AddBackward0>)\n",
      "loss:  5.1159076974727213e-11\n",
      "iteration 148, w1=tensor([2.0000], requires_grad=True), y=tensor([4.7424], grad_fn=<AddBackward0>)\n",
      "loss:  4.4565240386873484e-11\n",
      "iteration 149, w1=tensor([2.0000], requires_grad=True), y=tensor([4.7424], grad_fn=<AddBackward0>)\n",
      "loss:  3.842615114990622e-11\n",
      "iteration 150, w1=tensor([2.0000], requires_grad=True), y=tensor([4.7424], grad_fn=<AddBackward0>)\n",
      "loss:  3.2741809263825417e-11\n",
      "iteration 151, w1=tensor([2.0000], requires_grad=True), y=tensor([4.7424], grad_fn=<AddBackward0>)\n",
      "loss:  3.2741809263825417e-11\n",
      "iteration 152, w1=tensor([2.0000], requires_grad=True), y=tensor([4.7424], grad_fn=<AddBackward0>)\n",
      "loss:  2.751221472863108e-11\n",
      "iteration 153, w1=tensor([2.0000], requires_grad=True), y=tensor([4.7424], grad_fn=<AddBackward0>)\n",
      "loss:  2.2737367544323206e-11\n",
      "iteration 154, w1=tensor([2.0000], requires_grad=True), y=tensor([4.7424], grad_fn=<AddBackward0>)\n",
      "loss:  2.2737367544323206e-11\n",
      "iteration 155, w1=tensor([2.0000], requires_grad=True), y=tensor([4.7424], grad_fn=<AddBackward0>)\n",
      "loss:  1.8417267710901797e-11\n",
      "iteration 156, w1=tensor([2.0000], requires_grad=True), y=tensor([4.7424], grad_fn=<AddBackward0>)\n",
      "loss:  1.4551915228366852e-11\n",
      "iteration 157, w1=tensor([2.0000], requires_grad=True), y=tensor([4.7424], grad_fn=<AddBackward0>)\n",
      "loss:  1.4551915228366852e-11\n",
      "iteration 158, w1=tensor([2.0000], requires_grad=True), y=tensor([4.7424], grad_fn=<AddBackward0>)\n",
      "loss:  1.1141310096718371e-11\n",
      "iteration 159, w1=tensor([2.0000], requires_grad=True), y=tensor([4.7424], grad_fn=<AddBackward0>)\n",
      "loss:  1.1141310096718371e-11\n",
      "iteration 160, w1=tensor([2.0000], requires_grad=True), y=tensor([4.7424], grad_fn=<AddBackward0>)\n",
      "loss:  1.1141310096718371e-11\n",
      "iteration 161, w1=tensor([2.0000], requires_grad=True), y=tensor([4.7424], grad_fn=<AddBackward0>)\n",
      "loss:  8.185452315956354e-12\n",
      "iteration 162, w1=tensor([2.0000], requires_grad=True), y=tensor([4.7424], grad_fn=<AddBackward0>)\n",
      "loss:  8.185452315956354e-12\n",
      "iteration 163, w1=tensor([2.0000], requires_grad=True), y=tensor([4.7424], grad_fn=<AddBackward0>)\n",
      "loss:  5.6843418860808015e-12\n",
      "iteration 164, w1=tensor([2.0000], requires_grad=True), y=tensor([4.7424], grad_fn=<AddBackward0>)\n",
      "loss:  5.6843418860808015e-12\n",
      "iteration 165, w1=tensor([2.0000], requires_grad=True), y=tensor([4.7424], grad_fn=<AddBackward0>)\n",
      "loss:  5.6843418860808015e-12\n",
      "iteration 166, w1=tensor([2.0000], requires_grad=True), y=tensor([4.7424], grad_fn=<AddBackward0>)\n",
      "loss:  5.6843418860808015e-12\n",
      "iteration 167, w1=tensor([2.0000], requires_grad=True), y=tensor([4.7424], grad_fn=<AddBackward0>)\n",
      "loss:  3.637978807091713e-12\n",
      "iteration 168, w1=tensor([2.0000], requires_grad=True), y=tensor([4.7424], grad_fn=<AddBackward0>)\n",
      "loss:  3.637978807091713e-12\n",
      "iteration 169, w1=tensor([2.0000], requires_grad=True), y=tensor([4.7424], grad_fn=<AddBackward0>)\n",
      "loss:  3.637978807091713e-12\n",
      "iteration 170, w1=tensor([2.0000], requires_grad=True), y=tensor([4.7424], grad_fn=<AddBackward0>)\n",
      "loss:  3.637978807091713e-12\n",
      "iteration 171, w1=tensor([2.0000], requires_grad=True), y=tensor([4.7424], grad_fn=<AddBackward0>)\n",
      "loss:  2.0463630789890885e-12\n",
      "iteration 172, w1=tensor([2.0000], requires_grad=True), y=tensor([4.7424], grad_fn=<AddBackward0>)\n",
      "loss:  2.0463630789890885e-12\n",
      "iteration 173, w1=tensor([2.0000], requires_grad=True), y=tensor([4.7424], grad_fn=<AddBackward0>)\n",
      "loss:  2.0463630789890885e-12\n",
      "iteration 174, w1=tensor([2.0000], requires_grad=True), y=tensor([4.7424], grad_fn=<AddBackward0>)\n",
      "loss:  2.0463630789890885e-12\n",
      "iteration 175, w1=tensor([2.0000], requires_grad=True), y=tensor([4.7424], grad_fn=<AddBackward0>)\n",
      "loss:  2.0463630789890885e-12\n",
      "iteration 176, w1=tensor([2.0000], requires_grad=True), y=tensor([4.7424], grad_fn=<AddBackward0>)\n",
      "loss:  9.094947017729282e-13\n",
      "iteration 177, w1=tensor([2.0000], requires_grad=True), y=tensor([4.7424], grad_fn=<AddBackward0>)\n",
      "loss:  9.094947017729282e-13\n",
      "iteration 178, w1=tensor([2.0000], requires_grad=True), y=tensor([4.7424], grad_fn=<AddBackward0>)\n",
      "loss:  9.094947017729282e-13\n",
      "iteration 179, w1=tensor([2.0000], requires_grad=True), y=tensor([4.7424], grad_fn=<AddBackward0>)\n",
      "loss:  9.094947017729282e-13\n",
      "iteration 180, w1=tensor([2.0000], requires_grad=True), y=tensor([4.7424], grad_fn=<AddBackward0>)\n",
      "loss:  9.094947017729282e-13\n",
      "iteration 181, w1=tensor([2.0000], requires_grad=True), y=tensor([4.7424], grad_fn=<AddBackward0>)\n",
      "loss:  9.094947017729282e-13\n",
      "iteration 182, w1=tensor([2.0000], requires_grad=True), y=tensor([4.7424], grad_fn=<AddBackward0>)\n",
      "loss:  9.094947017729282e-13\n",
      "iteration 183, w1=tensor([2.0000], requires_grad=True), y=tensor([4.7424], grad_fn=<AddBackward0>)\n",
      "loss:  9.094947017729282e-13\n",
      "iteration 184, w1=tensor([2.0000], requires_grad=True), y=tensor([4.7424], grad_fn=<AddBackward0>)\n",
      "loss:  9.094947017729282e-13\n",
      "iteration 185, w1=tensor([2.0000], requires_grad=True), y=tensor([4.7424], grad_fn=<AddBackward0>)\n",
      "loss:  9.094947017729282e-13\n",
      "iteration 186, w1=tensor([2.0000], requires_grad=True), y=tensor([4.7424], grad_fn=<AddBackward0>)\n",
      "loss:  9.094947017729282e-13\n",
      "iteration 187, w1=tensor([2.0000], requires_grad=True), y=tensor([4.7424], grad_fn=<AddBackward0>)\n",
      "loss:  9.094947017729282e-13\n",
      "iteration 188, w1=tensor([2.0000], requires_grad=True), y=tensor([4.7424], grad_fn=<AddBackward0>)\n",
      "loss:  2.2737367544323206e-13\n",
      "iteration 189, w1=tensor([2.0000], requires_grad=True), y=tensor([4.7424], grad_fn=<AddBackward0>)\n",
      "loss:  2.2737367544323206e-13\n",
      "iteration 190, w1=tensor([2.0000], requires_grad=True), y=tensor([4.7424], grad_fn=<AddBackward0>)\n",
      "loss:  2.2737367544323206e-13\n",
      "iteration 191, w1=tensor([2.0000], requires_grad=True), y=tensor([4.7424], grad_fn=<AddBackward0>)\n",
      "loss:  2.2737367544323206e-13\n",
      "iteration 192, w1=tensor([2.0000], requires_grad=True), y=tensor([4.7424], grad_fn=<AddBackward0>)\n",
      "loss:  2.2737367544323206e-13\n",
      "iteration 193, w1=tensor([2.0000], requires_grad=True), y=tensor([4.7424], grad_fn=<AddBackward0>)\n",
      "loss:  2.2737367544323206e-13\n",
      "iteration 194, w1=tensor([2.0000], requires_grad=True), y=tensor([4.7424], grad_fn=<AddBackward0>)\n",
      "loss:  2.2737367544323206e-13\n",
      "iteration 195, w1=tensor([2.0000], requires_grad=True), y=tensor([4.7424], grad_fn=<AddBackward0>)\n",
      "loss:  2.2737367544323206e-13\n",
      "iteration 196, w1=tensor([2.0000], requires_grad=True), y=tensor([4.7424], grad_fn=<AddBackward0>)\n",
      "loss:  2.2737367544323206e-13\n",
      "iteration 197, w1=tensor([2.0000], requires_grad=True), y=tensor([4.7424], grad_fn=<AddBackward0>)\n",
      "loss:  2.2737367544323206e-13\n",
      "iteration 198, w1=tensor([2.0000], requires_grad=True), y=tensor([4.7424], grad_fn=<AddBackward0>)\n",
      "loss:  2.2737367544323206e-13\n",
      "iteration 199, w1=tensor([2.], requires_grad=True), y=tensor([4.7424], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y_pred = []\n",
    "EPOHS = 200\n",
    "for i in range(EPOHS):\n",
    "    optimizer.zero_grad()\n",
    "    pred = func(w1, x[0]) # если передавать весь тензор x, а не первый результат \n",
    "                          # появляется ошибка в  loss.backward():\n",
    "                          # \"element 0 of tensors does not require grad and does not have a grad_fn\"\n",
    "    loss = criterion(pred, y[0])\n",
    "    print('loss: ', loss.item())\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(f'iteration {i}, {w1=}, y={func(w1, x[0])}')\n",
    "    y_pred.append(func(w1, x[0]).detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1b5db07e700>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(y_pred, label='predictions')\n",
    "plt.axhline(y=y[0], color='r', linestyle='-', label='y real')\n",
    "plt.xlabel('iteration', fontsize=12, color='blue')\n",
    "plt.ylabel('y_pred', fontsize=12, color='blue')\n",
    "plt.title('оптимизация функции')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
