{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PyTorch HW_03.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN7VYGxMltBjZ4kMJzd7FzE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dave502/PyTorch/blob/main/HW_03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Создать Dataset для загрузки данных (используем только числовые данные)\n",
        "2. Обернуть его в Dataloader\n",
        "3. Написать архитектуру сети, которая предсказывает число показов на основании числовых данных (вы всегда можете нагенерить дополнительных факторов). Сеть должна включать BatchNorm слои и Dropout (или НЕ включать, но нужно обосновать)\n",
        "4. Учить будем на функцию потерь с кагла (log RMSE) - нужно её реализовать\n",
        "5. Сравните сходимость Adam, RMSProp и SGD, сделайте вывод по качеству работы модели\n",
        "\n",
        "train-test разделение нужно сделать с помощью sklearn random_state=13, test_size = 0.25\n",
        "\n",
        "Вопросы? в личку @Kinetikm"
      ],
      "metadata": {
        "id": "bKW92GGyUXwH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install --upgrade wandb\n",
        "# !pip install --upgrade git+https://github.com/PytorchLightning/pytorch-lightning.git"
      ],
      "metadata": {
        "id": "tHAuBPJ0jEnp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQM3WupmwgaS",
        "outputId": "2ead804a-47dc-4448-d0f6-520d6c3c327b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "from torch import Tensor\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import namedtuple\n",
        "from sklearn.model_selection import train_test_split\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "from sklearn.preprocessing import LabelEncoder"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_path = \"/content/drive/MyDrive/MLData/train.csv\""
      ],
      "metadata": {
        "id": "gMYy4HeIwsyF"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*код загрузки датасета писался для Jupyter, для колаба я уже не стал его переписывать*"
      ],
      "metadata": {
        "id": "BH4X71fPYBfW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %%capture\n",
        "# !pip install kaggle\n",
        "# from kaggle.api.kaggle_api_extended import KaggleApi\n",
        "# api = KaggleApi()\n",
        "# api.authenticate()\n",
        "# api.competition_download_file('avito-demand-prediction', 'train.csv', path='data/')"
      ],
      "metadata": {
        "id": "gyBINckqUwx9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(df_path)\n",
        "X_train, X_test = train_test_split(data, test_size = 0.25, random_state=13)\n",
        "print(f'X_train.shape={X_train.shape} X_test.shape={X_test.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oEkIMfaJJOg5",
        "outputId": "5f0b0cdd-b594-4e4d-b9c3-05bbd16fa793"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train.shape=(1127568, 18) X_test.shape=(375856, 18)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.categorical_features = [\"region\", \"city\", \"parent_category_name\", \"category_name\"]\n",
        "        self.numerics_features = data.select_dtypes(include=np.number).columns.tolist()\n",
        "        \n",
        "        # категориальные признаки\n",
        "        label_encoders = {}\n",
        "        for cat_col in self.categorical_features:\n",
        "          label_encoders[cat_col] = LabelEncoder()\n",
        "          data[cat_col] = label_encoders[cat_col].fit_transform(data[cat_col])\n",
        "\n",
        "        # числовые признаки\n",
        "        # запоняем пропуски в числовых колонках\n",
        "        data[self.numerics_features] = data[self.numerics_features].apply(pd.to_numeric, errors='coerce')\n",
        "        # data[self.numerics_features].fillna(0, inplace=True)\n",
        "\n",
        "        # разделяем обучающий датасет на фичи и целевую переменную \n",
        "        self.X_num_train = torch.FloatTensor(data[self.numerics_features].values)\n",
        "        self.X_num_train[self.X_num_train!=self.X_num_train]=0 # fillna почему-то не заполяет nans, такой способ более оказался эффективным\n",
        "\n",
        "        self.X_cat_train = torch.IntTensor(data[self.categorical_features].values)\n",
        "        self.y_train = torch.FloatTensor(data['deal_probability'])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y_train)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.X_num_train[index], self.X_cat_train[index], self.y_train[index]\n",
        "\n",
        "    def embedings_dim(self):\n",
        "      \"\"\"\n",
        "      возвращает количество и размерность эмбеддигов\n",
        "      \"\"\"\n",
        "      cat_dims = [torch.unique(self.X_cat_train[:,i]).size()[0] for i,_ in enumerate(self.categorical_features)]\n",
        "      embedings_dims = [(x, min(50, (x + 1) // 2)) for x in cat_dims]\n",
        "      return embedings_dims\n",
        "\n",
        "    def numeric_dim(self):\n",
        "      \"\"\"\n",
        "      возвращает количество числовых фичей (input size для модели)\n",
        "      \"\"\"\n",
        "      return self.X_num_train.size()[1]\n"
      ],
      "metadata": {
        "id": "ssXvH8EZxLeM"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = Dataset(X_train)"
      ],
      "metadata": {
        "id": "PnPLpUwj2R1M"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(dataset=train_ds, batch_size=32, num_workers=10, drop_last=True)"
      ],
      "metadata": {
        "id": "K63xtZC9xU3Q"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "код нейронной сети со второго дз с небольшими изменениями состава слоёв"
      ],
      "metadata": {
        "id": "qLzp5b62Zpja"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Perceptron(nn.Module):\n",
        "    \n",
        "    # tuple с активационными функциями (tuple для неизменности)\n",
        "    Activatons = namedtuple('Activatons', 'relu silu leaky_relu sigmoid softmax')\n",
        "    activations = Activatons(F.relu, F.silu, F.leaky_relu, F.sigmoid, F.softmax)\n",
        "    \n",
        "    def __init__(self, input_dim, output_dim, activation='relu', activation_params=[]):\n",
        "        super(Perceptron, self).__init__()\n",
        "        #fc - полносвязный слой\n",
        "        self.fc = nn.Linear(input_dim, output_dim)\n",
        "        #ac - функция активации\n",
        "        self.ac = getattr(self.activations, activation)\n",
        "        #ac_params - параметры функции активации\n",
        "        self.ac_params = activation_params\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc(x)\n",
        "        return self.ac(x, *self.ac_params)"
      ],
      "metadata": {
        "id": "O24BAkvQxYxC"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, num_dim, emb_dim, hidden_dim, output_dim):\n",
        "        super(Model, self).__init__()\n",
        "\n",
        "        # эмбеддинг слой для каждого категориального признака\n",
        "        self.cat_layers = nn.ModuleList([\n",
        "            nn.Embedding(x, y) for x, y in emb_dim\n",
        "        ])\n",
        "\n",
        "        # слой нормализации для числовых признаков   \n",
        "        self.num_layers = nn.ModuleList([\n",
        "            nn.BatchNorm1d(num_dim), \n",
        "        ])   \n",
        "\n",
        "        # расчет суммарной выходной размерности слоёв категориальных и числовых признаков\n",
        "        total_input = sum([x[1] for x in emb_dim]) +  num_dim                          \n",
        "\n",
        "        # общие слои\n",
        "        self.next_layers = nn.ModuleList([\n",
        "            Perceptron(total_input, 4*hidden_dim, \"silu\"),\n",
        "            nn.BatchNorm1d(4*hidden_dim),\n",
        "            nn.Dropout(0.25),\n",
        "            Perceptron(4*hidden_dim, 4*hidden_dim, \"leaky_relu\"),\n",
        "            nn.BatchNorm1d(4*hidden_dim),\n",
        "            nn.Dropout(0.25),\n",
        "            Perceptron(4*hidden_dim, output_dim, \"sigmoid\"),\n",
        "        ])\n",
        "        \n",
        "        \n",
        "    def forward(self, x_num, x_cat):\n",
        "\n",
        "        # эмбеддинг слои\n",
        "        x = [emb_layer(x_cat[:, i])\n",
        "           for i, emb_layer in enumerate(self.cat_layers)]\n",
        "        # объединение выходов всех эмбеддинг слоёв\n",
        "        x = torch.cat(x, 1)\n",
        "\n",
        "        # числовые слои\n",
        "        for layer in self.num_layers:\n",
        "            x_num = layer.forward(x_num) \n",
        "\n",
        "        # объединение выходов слоёв для числовых признаков и для категориальных\n",
        "        x = torch.cat([x, x_num], 1)     \n",
        "\n",
        "        # дальше общие скрытые слои\n",
        "        for layer in self.next_layers:\n",
        "            x = layer.forward(x) \n",
        "        return x \n",
        "    \n",
        "    # def predict(self, x):\n",
        "    #     for layer in self.layers:\n",
        "    #         x = layer.forward(x) \n",
        "    #     return x"
      ],
      "metadata": {
        "id": "zFqaVaGlxbsa"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "log RMSE"
      ],
      "metadata": {
        "id": "PKkOCQxEa53i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LogRMSE(torch.nn.modules.loss._Loss):\n",
        "    def __init__(self, reduction: str = 'none') -> None:\n",
        "        super(LogRMSE, self).__init__(None, None, reduction)\n",
        "    \n",
        "    def forward(self, input: Tensor, target: Tensor) -> Tensor:\n",
        "        se = torch.pow((target - input), 2)\n",
        "        mse = torch.mean(se)\n",
        "        rmse = torch.sqrt(mse)\n",
        "        logrmse = torch.log(rmse)\n",
        "        return logrmse\n"
      ],
      "metadata": {
        "id": "eLH4Z7uFxeK_"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Model(train_ds.numeric_dim(), train_ds.embedings_dim(), 200, 1)\n",
        "\n",
        "optimizers = {'SGD': torch.optim.SGD(model.parameters(), lr=0.01),\n",
        "              'Adam': torch.optim.Adam(model.parameters(), lr=0.01),\n",
        "              'RMSProp': torch.optim.RMSprop(model.parameters(), lr=0.01),\n",
        "}\n",
        "criterion = LogRMSE()"
      ],
      "metadata": {
        "id": "UhdBN1hNxgY6"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for optimizer_name, optimizer in optimizers.items():\n",
        "  print(f'-------------optimizer {optimizer_name}-------------\\n')\n",
        "  EPOCHS = 5\n",
        "  for epoch in tqdm(range(EPOCHS)):  \n",
        "      running_loss = 0.0\n",
        "      for i, data in enumerate(train_loader, 0):\n",
        "          inputs_num, inputs_cat, labels = data[0], data[1], data[2]\n",
        "\n",
        "          # обнуляем градиент\n",
        "          optimizer.zero_grad()\n",
        "          outputs = model(inputs_num, inputs_cat)\n",
        "          loss = criterion(outputs, labels)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          # выводим статистику о процессе обучения\n",
        "          running_loss += loss.item()\n",
        "          if i % 5000 == 0:    # печатаем каждые 5000 mini-batches\n",
        "              print('[%d, %5d] loss: %.3f' %\n",
        "                    (epoch + 1, i + 1, running_loss / 5000))\n",
        "              running_loss = 0.0\n",
        "  print('\\nTraining is finished!')"
      ],
      "metadata": {
        "id": "NHMguIuh1TG7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2aae4bfa-ab81-47d6-b0a4-3edd7a0542c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------------optimizer SGD-------------\n",
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/5 [00:00<?, ?it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1,     1] loss: -0.000\n",
            "[1,  5001] loss: -1.369\n",
            "[1, 10001] loss: -1.383\n",
            "[1, 15001] loss: -1.384\n",
            "[1, 20001] loss: -1.386\n",
            "[1, 25001] loss: -1.389\n",
            "[1, 30001] loss: -1.387\n",
            "[1, 35001] loss: -1.385\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 20%|██        | 1/5 [04:24<17:39, 264.94s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2,     1] loss: -0.000\n",
            "[2,  5001] loss: -1.392\n",
            "[2, 10001] loss: -1.383\n",
            "[2, 15001] loss: -1.384\n",
            "[2, 20001] loss: -1.386\n",
            "[2, 25001] loss: -1.389\n",
            "[2, 30001] loss: -1.387\n",
            "[2, 35001] loss: -1.385\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 40%|████      | 2/5 [08:41<13:00, 260.29s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3,     1] loss: -0.000\n",
            "[3,  5001] loss: -1.392\n",
            "[3, 10001] loss: -1.383\n",
            "[3, 15001] loss: -1.384\n",
            "[3, 20001] loss: -1.386\n",
            "[3, 25001] loss: -1.389\n",
            "[3, 35001] loss: -1.385\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|██████    | 3/5 [12:55<08:33, 256.99s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4,     1] loss: -0.000\n",
            "[4,  5001] loss: -1.392\n",
            "[4, 10001] loss: -1.383\n",
            "[4, 15001] loss: -1.384\n",
            "[4, 20001] loss: -1.386\n",
            "[4, 25001] loss: -1.389\n",
            "[4, 30001] loss: -1.387\n",
            "[4, 35001] loss: -1.385\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|████████  | 4/5 [17:09<04:16, 256.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5,     1] loss: -0.000\n",
            "[5,  5001] loss: -1.392\n",
            "[5, 10001] loss: -1.383\n",
            "[5, 15001] loss: -1.384\n",
            "[5, 20001] loss: -1.386\n",
            "[5, 25001] loss: -1.389\n",
            "[5, 30001] loss: -1.387\n",
            "[5, 35001] loss: -1.385\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5/5 [21:22<00:00, 256.43s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training is finished!\n",
            "-------------optimizer Adam-------------\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/5 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,     1] loss: -0.000\n",
            "[1,  5001] loss: -1.379\n",
            "[1, 10001] loss: -1.377\n",
            "[1, 15001] loss: -1.380\n",
            "[1, 20001] loss: -1.382\n",
            "[1, 25001] loss: -1.384\n",
            "[1, 30001] loss: -1.381\n",
            "[1, 35001] loss: -1.381\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|██        | 1/5 [06:42<26:51, 402.76s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2,     1] loss: -0.000\n",
            "[2,  5001] loss: -1.386\n",
            "[2, 10001] loss: -1.378\n",
            "[2, 15001] loss: -1.379\n",
            "[2, 20001] loss: -1.382\n",
            "[2, 25001] loss: -1.383\n",
            "[2, 30001] loss: -1.381\n",
            "[2, 35001] loss: -1.381\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 2/5 [14:14<21:34, 431.43s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3,     1] loss: -0.000\n",
            "[3,  5001] loss: -1.387\n",
            "[3, 10001] loss: -1.378\n",
            "[3, 15001] loss: -1.380\n",
            "[3, 20001] loss: -1.382\n",
            "[3, 25001] loss: -1.384\n",
            "[3, 30001] loss: -1.381\n",
            "[3, 35001] loss: -1.381\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|██████    | 3/5 [21:34<14:30, 435.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4,     1] loss: -0.000\n",
            "[4,  5001] loss: -1.386\n",
            "[4, 10001] loss: -1.377\n",
            "[4, 15001] loss: -1.380\n",
            "[4, 20001] loss: -1.382\n",
            "[4, 25001] loss: -1.383\n",
            "[4, 30001] loss: -1.381\n",
            "[4, 35001] loss: -1.381\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|████████  | 4/5 [28:48<07:14, 434.87s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5,     1] loss: -0.000\n",
            "[5,  5001] loss: -1.386\n",
            "[5, 10001] loss: -1.378\n",
            "[5, 15001] loss: -1.380\n",
            "[5, 20001] loss: -1.382\n",
            "[5, 25001] loss: -1.383\n",
            "[5, 30001] loss: -1.337\n",
            "[5, 35001] loss: -1.275\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5/5 [35:48<00:00, 429.62s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training is finished!\n",
            "-------------optimizer RMSProp-------------\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/5 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,     1] loss: -0.000\n",
            "[1,  5001] loss: -1.283\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*RMSProp не доработал до конца, но результаты понятны...*"
      ],
      "metadata": {
        "id": "MYQkzIcZc9xN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adam и SGD почти не отличаются по метрике, а RMSProp показал результаты немного хуже. Выводы относятся, конечно же к оптимизаторам \"из коробки\", без дополнительной настройки."
      ],
      "metadata": {
        "id": "gn9kH603cXSd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Модель почему-то не обучается. Возможно, это нормальное поведение, но подозреваю, что где-то в коде баг. Визуально вроде всё ок, а отладка займёт много времени. Уж очень затрантное по времени дз в связи с размером датасета.    "
      ],
      "metadata": {
        "id": "SFykxFQRdUFN"
      }
    }
  ]
}