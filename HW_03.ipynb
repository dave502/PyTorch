{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PyTorch HW_03.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNERsMytzzSun6aWmKZRKKM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dave502/PyTorch/blob/main/HW_03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Создать Dataset для загрузки данных (используем только числовые данные)\n",
        "2. Обернуть его в Dataloader\n",
        "3. Написать архитектуру сети, которая предсказывает число показов на основании числовых данных (вы всегда можете нагенерить дополнительных факторов). Сеть должна включать BatchNorm слои и Dropout (или НЕ включать, но нужно обосновать)\n",
        "4. Учить будем на функцию потерь с кагла (log RMSE) - нужно её реализовать\n",
        "5. Сравните сходимость Adam, RMSProp и SGD, сделайте вывод по качеству работы модели\n",
        "\n",
        "train-test разделение нужно сделать с помощью sklearn random_state=13, test_size = 0.25\n",
        "\n",
        "Вопросы? в личку @Kinetikm"
      ],
      "metadata": {
        "id": "bKW92GGyUXwH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install --upgrade wandb\n",
        "# !pip install --upgrade git+https://github.com/PytorchLightning/pytorch-lightning.git"
      ],
      "metadata": {
        "id": "tHAuBPJ0jEnp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQM3WupmwgaS",
        "outputId": "df70ffad-d48f-4fcb-d87a-148007d95cf3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "from torch import Tensor\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import namedtuple\n",
        "from sklearn.model_selection import train_test_split\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# df_path = \"data/df_train.csv\"\n",
        "df_path = \"/content/drive/MyDrive/MLData/df_train.csv\""
      ],
      "metadata": {
        "id": "gMYy4HeIwsyF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Этот код загрузки датасета и разделения его на обучающий и тестовый писался для Jupyter, для колаба я уже не стал его переписывать*"
      ],
      "metadata": {
        "id": "BH4X71fPYBfW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %%capture\n",
        "# !pip install kaggle\n",
        "# from kaggle.api.kaggle_api_extended import KaggleApi\n",
        "# api = KaggleApi()\n",
        "# api.authenticate()\n",
        "# api.competition_download_file('avito-demand-prediction', 'train.csv', path='data/')"
      ],
      "metadata": {
        "id": "gyBINckqUwx9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# if not Path(df_path).is_file():\n",
        "#     data = pd.read_csv('data/train.csv.zip')\n",
        "    \n",
        "#     X_train, X_test = train_test_split(data, test_size = 0.25, random_state=13)\n",
        "#     print(f'{X_train.shape=} {X_test.shape=}')\n",
        "\n",
        "#     cat_columns=['region', 'parent_category_name', 'user_type']\n",
        "#     X_train_mod = pd.get_dummies(X_train, columns=cat_columns) \n",
        "    \n",
        "#     X_train_mod.to_csv(df_path)"
      ],
      "metadata": {
        "id": "yG6-mNJuU_Vj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, path):\n",
        "        data = pd.read_csv(path)\n",
        "        # находим числовые колонки\n",
        "        numerics_colnames = data.select_dtypes(include=np.number).columns.tolist()\n",
        "        # убираем текстовые колонки\n",
        "        data.drop(data.columns.difference(numerics_colnames), 1, inplace=True)\n",
        "        # запоняем пропуски в числовых колонках\n",
        "        data.apply(pd.to_numeric, errors='coerce')\n",
        "        data.fillna(0, inplace=True)\n",
        "        # разделяем обучающий датасет на фичи и целевую переменную \n",
        "        self.X_train = torch.FloatTensor(data[data.columns.difference(['deal_probability'])].values)\n",
        "        self.y_train = torch.FloatTensor(data['deal_probability'])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y_train)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.X_train[index],  self.y_train[index]\n",
        "\n",
        "    def dim(self):\n",
        "      \"\"\"\n",
        "      возвращает количество фичей (input size для модели)\n",
        "      \"\"\"\n",
        "        return self.X_train.size()[1]    "
      ],
      "metadata": {
        "id": "ssXvH8EZxLeM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = Dataset(df_path)"
      ],
      "metadata": {
        "id": "PnPLpUwj2R1M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(dataset=train_ds, batch_size=64, num_workers=10, drop_last=True)"
      ],
      "metadata": {
        "id": "K63xtZC9xU3Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "код нейронной сети со второго дз с небольшими изменениями состава слоёв"
      ],
      "metadata": {
        "id": "qLzp5b62Zpja"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Perceptron(nn.Module):\n",
        "    \n",
        "    # tuple с активационными функциями (tuple для неизменности)\n",
        "    Activatons = namedtuple('Activatons', 'relu silu leaky_relu sigmoid softmax')\n",
        "    activations = Activatons(F.relu, F.silu, F.leaky_relu, F.sigmoid, F.softmax)\n",
        "    \n",
        "    def __init__(self, input_dim, output_dim, activation='relu', activation_params=[]):\n",
        "        super(Perceptron, self).__init__()\n",
        "        #fc - полносвязный слой\n",
        "        self.fc = nn.Linear(input_dim, output_dim)\n",
        "        #ac - функция активации\n",
        "        self.ac = getattr(self.activations, activation)\n",
        "        #ac_params - параметры функции активации\n",
        "        self.ac_params = activation_params\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc(x)\n",
        "        return self.ac(x, *self.ac_params)"
      ],
      "metadata": {
        "id": "O24BAkvQxYxC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(Model, self).__init__()\n",
        "\n",
        "        self.layers = nn.ModuleList([\n",
        "            nn.BatchNorm1d(input_dim),\n",
        "            Perceptron(input_dim, 4*hidden_dim, \"silu\"),\n",
        "            nn.BatchNorm1d(4*hidden_dim),\n",
        "            nn.Dropout(0.25),\n",
        "            Perceptron(4*hidden_dim, 4*hidden_dim, \"leaky_relu\"),\n",
        "            nn.BatchNorm1d(4*hidden_dim),\n",
        "            nn.Dropout(0.25),\n",
        "            Perceptron(4*hidden_dim, output_dim, \"sigmoid\"),\n",
        "        ])\n",
        "        \n",
        "        \n",
        "    def forward(self, x):\n",
        "        for layer in self.layers:\n",
        "            x = layer.forward(x) \n",
        "        return x \n",
        "    \n",
        "    # def predict(self, x):\n",
        "    #     for layer in self.layers:\n",
        "    #         x = layer.forward(x) \n",
        "    #     return x"
      ],
      "metadata": {
        "id": "zFqaVaGlxbsa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "log RMSE"
      ],
      "metadata": {
        "id": "PKkOCQxEa53i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LogRMSE(torch.nn.modules.loss._Loss):\n",
        "    def __init__(self, reduction: str = 'none') -> None:\n",
        "        super(LogRMSE, self).__init__(None, None, reduction)\n",
        "    \n",
        "    def forward(self, input: Tensor, target: Tensor) -> Tensor:\n",
        "        se = torch.pow((target - input), 2)\n",
        "        mse = torch.mean(se)\n",
        "        rmse = torch.sqrt(mse)\n",
        "        logrmse = torch.log(rmse)\n",
        "        return logrmse\n"
      ],
      "metadata": {
        "id": "eLH4Z7uFxeK_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Model(train_ds.dim(), 200, 1)\n",
        "\n",
        "optimizers = {'SGD': torch.optim.SGD(model.parameters(), lr=0.01),\n",
        "              'Adam': torch.optim.Adam(model.parameters(), lr=0.01),\n",
        "              'RMSProp': torch.optim.RMSprop(model.parameters(), lr=0.01),\n",
        "}\n",
        "criterion = LogRMSE()"
      ],
      "metadata": {
        "id": "UhdBN1hNxgY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for optimizer_name, optimizer in optimizers.items():\n",
        "  print(f'-------------optimizer {optimizer_name}-------------\\n')\n",
        "  EPOCHS = 5\n",
        "  for epoch in tqdm(range(EPOCHS)):  \n",
        "      running_loss = 0.0\n",
        "      for i, data in enumerate(train_loader, 0):\n",
        "          inputs, labels = data[0], data[1]\n",
        "\n",
        "          # обнуляем градиент\n",
        "          optimizer.zero_grad()\n",
        "          outputs = model(inputs)\n",
        "          loss = criterion(outputs, labels)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          # выводим статистику о процессе обучения\n",
        "          running_loss += loss.item()\n",
        "          if i % 5000 == 0:    # печатаем каждые 5000 mini-batches\n",
        "              print('[%d, %5d] loss: %.3f' %\n",
        "                    (epoch + 1, i + 1, running_loss / 5000))\n",
        "              running_loss = 0.0\n",
        "  print('\\nTraining is finished!')"
      ],
      "metadata": {
        "id": "NHMguIuh1TG7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dccc5385-5476-4da3-b41e-f3affb0891df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------optimizer SGD-------------\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/5 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,     1] loss: -0.000\n",
            "[1,  5001] loss: -1.345\n",
            "[1, 10001] loss: -1.365\n",
            "[1, 15001] loss: -1.366\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|██        | 1/5 [02:53<11:32, 173.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2,     1] loss: -0.000\n",
            "[2,  5001] loss: -1.366\n",
            "[2, 10001] loss: -1.365\n",
            "[2, 15001] loss: -1.366\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 2/5 [05:30<08:11, 163.94s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3,     1] loss: -0.000\n",
            "[3,  5001] loss: -1.366\n",
            "[3, 10001] loss: -1.365\n",
            "[3, 15001] loss: -1.366\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|██████    | 3/5 [08:04<05:18, 159.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4,     1] loss: -0.000\n",
            "[4,  5001] loss: -1.366\n",
            "[4, 10001] loss: -1.365\n",
            "[4, 15001] loss: -1.366\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|████████  | 4/5 [10:35<02:36, 156.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5,     1] loss: -0.000\n",
            "[5,  5001] loss: -1.366\n",
            "[5, 10001] loss: -1.365\n",
            "[5, 15001] loss: -1.366\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5/5 [13:10<00:00, 158.07s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training is finished!\n",
            "-------------optimizer Adam-------------\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/5 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,     1] loss: -0.000\n",
            "[1,  5001] loss: -1.359\n",
            "[1, 10001] loss: -1.362\n",
            "[1, 15001] loss: -1.364\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|██        | 1/5 [03:22<13:31, 202.88s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2,     1] loss: -0.000\n",
            "[2,  5001] loss: -1.364\n",
            "[2, 10001] loss: -1.363\n",
            "[2, 15001] loss: -1.364\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 2/5 [06:44<10:05, 201.96s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3,     1] loss: -0.000\n",
            "[3,  5001] loss: -1.364\n",
            "[3, 10001] loss: -1.363\n",
            "[3, 15001] loss: -1.364\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|██████    | 3/5 [10:41<07:16, 218.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4,     1] loss: -0.000\n",
            "[4,  5001] loss: -1.364\n",
            "[4, 10001] loss: -1.363\n",
            "[4, 15001] loss: -1.364\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|████████  | 4/5 [14:29<03:41, 221.88s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5,     1] loss: -0.000\n",
            "[5,  5001] loss: -1.364\n",
            "[5, 10001] loss: -1.363\n",
            "[5, 15001] loss: -1.364\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5/5 [18:29<00:00, 221.81s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training is finished!\n",
            "-------------optimizer RMSProp-------------\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/5 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,     1] loss: -0.000\n",
            "[1,  5001] loss: -1.247\n",
            "[1, 10001] loss: -1.245\n",
            "[1, 15001] loss: -1.247\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|██        | 1/5 [08:01<32:07, 481.88s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2,     1] loss: -0.000\n",
            "[2,  5001] loss: -1.247\n",
            "[2, 10001] loss: -1.245\n",
            "[2, 15001] loss: -1.247\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 2/5 [28:08<45:24, 908.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3,     1] loss: -0.000\n",
            "[3,  5001] loss: -1.247\n",
            "[3, 10001] loss: -1.245\n",
            "[3, 15001] loss: -1.247\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|██████    | 3/5 [48:27<35:00, 1050.36s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4,     1] loss: -0.000\n",
            "[4,  5001] loss: -1.247\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "CKlXZhvcGL44"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}